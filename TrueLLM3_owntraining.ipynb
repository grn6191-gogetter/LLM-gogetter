{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12f5b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1: Data Extraction from User-Provided Files (PDF, Word, Excel, Powerpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986b7433",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install PyPDF2 python-docx openpyxl python-pptx in anaconda prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3259b0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import PyPDF2\n",
    "import docx\n",
    "import openpyxl\n",
    "from pptx import Presentation\n",
    "import re\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "# 1. Function to extract text from PDF files\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = ''\n",
    "    try:\n",
    "        with open(pdf_path, 'rb') as file:\n",
    "            reader = PyPDF2.PdfReader(file)\n",
    "            for page_num in range(len(reader.pages)):\n",
    "                page = reader.pages[page_num]\n",
    "                text += page.extract_text() + \"\\n\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading PDF file {pdf_path}: {e}\")\n",
    "    return text\n",
    "\n",
    "# 2. Function to extract text from Word files\n",
    "def extract_text_from_word(docx_path):\n",
    "    text = ''\n",
    "    try:\n",
    "        doc = docx.Document(docx_path)\n",
    "        for para in doc.paragraphs:\n",
    "            text += para.text + \"\\n\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading Word file {docx_path}: {e}\")\n",
    "    return text\n",
    "\n",
    "# 3. Function to extract text from Excel files\n",
    "def extract_text_from_excel(excel_path):\n",
    "    text = ''\n",
    "    try:\n",
    "        wb = openpyxl.load_workbook(excel_path)\n",
    "        for sheet_name in wb.sheetnames:\n",
    "            sheet = wb[sheet_name]\n",
    "            for row in sheet.iter_rows(values_only=True):\n",
    "                text += ' '.join([str(cell) for cell in row if cell is not None]) + \"\\n\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading Excel file {excel_path}: {e}\")\n",
    "    return text\n",
    "\n",
    "# 4. Function to extract text from PowerPoint files\n",
    "def extract_text_from_ppt(ppt_path):\n",
    "    text = ''\n",
    "    try:\n",
    "        prs = Presentation(ppt_path)\n",
    "        for slide in prs.slides:\n",
    "            for shape in slide.shapes:\n",
    "                if hasattr(shape, \"text\"):\n",
    "                    text += shape.text + \"\\n\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading PowerPoint file {ppt_path}: {e}\")\n",
    "    return text\n",
    "\n",
    "# Unified function to handle different file types\n",
    "def extract_text_from_file(file_path):\n",
    "    extension = os.path.splitext(file_path)[1].lower()\n",
    "    \n",
    "    if extension == '.pdf':\n",
    "        return extract_text_from_pdf(file_path)\n",
    "    elif extension == '.docx':\n",
    "        return extract_text_from_word(file_path)\n",
    "    elif extension == '.xlsx':\n",
    "        return extract_text_from_excel(file_path)\n",
    "    elif extension == '.pptx':\n",
    "        return extract_text_from_ppt(file_path)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file format: {extension}\")\n",
    "\n",
    "# Clean and preprocess the extracted text\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'<.*?>', '', text)  # Remove HTML tags\n",
    "    text = re.sub(r'\\[.*?\\]', '', text)  # Remove square brackets content\n",
    "    text = re.sub(r'[^a-zA-Z0-9.,!?\\'\\s]', '', text)  # Remove special characters\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces with single space\n",
    "    return text.strip()\n",
    "\n",
    "# Example usage: Handling multiple files\n",
    "file_paths = [\n",
    "    \"path_to_file1.pdf\", \n",
    "    \"path_to_file2.docx\", \n",
    "    \"path_to_file3.xlsx\", \n",
    "    \"path_to_file4.pptx\"\n",
    "]\n",
    "\n",
    "corpus = ''\n",
    "for file_path in file_paths:\n",
    "    corpus += extract_text_from_file(file_path)\n",
    "\n",
    "cleaned_corpus = clean_text(corpus)\n",
    "\n",
    "print(\"Extracted and Cleaned Text:\", cleaned_corpus[:500])  # Print the first 500 characters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395bd81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26d13f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the tokenizer on the cleaned corpus\n",
    "def train_tokenizer_from_corpus(corpus, vocab_size=30000, min_frequency=2, save_path=\"./tokenizer\"):\n",
    "    # Save corpus to a temporary text file for tokenizer training\n",
    "    with open(\"corpus.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(corpus)\n",
    "    \n",
    "    tokenizer = ByteLevelBPETokenizer()\n",
    "    tokenizer.train(files=[\"corpus.txt\"], vocab_size=vocab_size, min_frequency=min_frequency)\n",
    "    tokenizer.save_model(save_path)\n",
    "    return tokenizer\n",
    "\n",
    "# Train tokenizer on the cleaned corpus\n",
    "tokenizer = train_tokenizer_from_corpus(cleaned_corpus)\n",
    "\n",
    "# Tokenize some sample text\n",
    "encoded = tokenizer.encode(\"Extracted data from multiple files.\")\n",
    "print(encoded.tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85365aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 3: Transformer based Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e219dcf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import math\n",
    "\n",
    "# Positional encoding for the transformer model\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return x\n",
    "\n",
    "# Transformer Model definition\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, max_seq_length):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, max_seq_length)\n",
    "        self.transformer = nn.Transformer(d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward)\n",
    "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src = self.embedding(src) * math.sqrt(self.d_model)\n",
    "        tgt = self.embedding(tgt) * math.sqrt(self.d_model)\n",
    "        src = self.pos_encoder(src)\n",
    "        tgt = self.pos_encoder(tgt)\n",
    "        output = self.transformer(src, tgt)\n",
    "        return self.fc_out(output)\n",
    "\n",
    "# Training setup\n",
    "def train_model(model, tokenizer, corpus, batch_size=32, epochs=10, max_seq_length=512):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Dummy data (since training from real data would be large-scale)\n",
    "    tokenized_sequences = [tokenizer.encode(corpus).ids]  # Tokenizing corpus\n",
    "    data_loader = torch.utils.data.DataLoader(tokenized_sequences, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch in data_loader:\n",
    "            # Prepare the data and pass it through the model\n",
    "            src = batch[:, :-1]  # Input tokens\n",
    "            tgt = batch[:, 1:]   # Target tokens (shifted by 1)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(src, tgt)\n",
    "            loss = criterion(output.view(-1, output.size(-1)), tgt.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {total_loss / len(data_loader)}\")\n",
    "\n",
    "# Example usage of model training\n",
    "vocab_size = tokenizer.get_vocab_size()\n",
    "d_model = 512\n",
    "nhead = 8\n",
    "num_encoder_layers = 6\n",
    "num_decoder_layers = 6\n",
    "dim_feedforward = 2048\n",
    "max_seq_length = 512\n",
    "\n",
    "model = TransformerModel(vocab_size, d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, max_seq_length)\n",
    "train_model(model, tokenizer, cleaned_corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7544890f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

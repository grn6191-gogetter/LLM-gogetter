{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "801166c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1: Data Extraction from User-Provided Files (PDF, Word, Excel, Powerpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4056b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install PyPDF2 python-docx openpyxl python-pptx in anaconda prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e093774",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install python-pptx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa530909",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tokenizers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b554cafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16f3ac2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu118\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77cc185a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03d8f8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install sentencepiece\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aac63c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d94cbd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f49581b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted and Cleaned Text: INTRODUCTION TO SPECTROSCOPY Donald L. Pavia Gary M. LampmanGeorge S. KrizJames R. Vyvyan Department of Chemistry Western Washington UniversityBellingham, WashingtonFIFTH EDITION Australia Brazil Mexico Singapore United Kingdom United States Copyright 2013 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. Copyright 2013 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. TOALL OF OUROS PEC STUDENTS Copyright 2013 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part. This is an electronic version of the print textbook. Due to electronic rights restrictions, some third party content may be suppressed. Editorial review has deemed that any suppressed content does not materially affect the overall learning experience. The publisher reserves the right to remove content from this title at any time if subsequent rights restrictions require it.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import PyPDF2\n",
    "import docx\n",
    "import openpyxl\n",
    "from pptx import Presentation\n",
    "import re\n",
    "\n",
    "\n",
    "# 1. Function to extract text from PDF files\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = ''\n",
    "    try:\n",
    "        with open(pdf_path, 'rb') as file:\n",
    "            reader = PyPDF2.PdfReader(file)\n",
    "            for page_num in range(len(reader.pages)):\n",
    "                page = reader.pages[page_num]\n",
    "                text += page.extract_text() + \"\\n\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading PDF file {pdf_path}: {e}\")\n",
    "    return text\n",
    "\n",
    "# 2. Function to extract text from Word files\n",
    "def extract_text_from_word(docx_path):\n",
    "    text = ''\n",
    "    try:\n",
    "        doc = docx.Document(docx_path)\n",
    "        for para in doc.paragraphs:\n",
    "            text += para.text + \"\\n\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading Word file {docx_path}: {e}\")\n",
    "    return text\n",
    "\n",
    "# 3. Function to extract text from Excel files\n",
    "def extract_text_from_excel(excel_path):\n",
    "    text = ''\n",
    "    try:\n",
    "        wb = openpyxl.load_workbook(excel_path)\n",
    "        for sheet_name in wb.sheetnames:\n",
    "            sheet = wb[sheet_name]\n",
    "            for row in sheet.iter_rows(values_only=True):\n",
    "                text += ' '.join([str(cell) for cell in row if cell is not None]) + \"\\n\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading Excel file {excel_path}: {e}\")\n",
    "    return text\n",
    "\n",
    "# 4. Function to extract text from PowerPoint files\n",
    "def extract_text_from_ppt(ppt_path):\n",
    "    text = ''\n",
    "    try:\n",
    "        prs = Presentation(ppt_path)\n",
    "        for slide in prs.slides:\n",
    "            for shape in slide.shapes:\n",
    "                if hasattr(shape, \"text\"):\n",
    "                    text += shape.text + \"\\n\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading PowerPoint file {ppt_path}: {e}\")\n",
    "    return text\n",
    "\n",
    "# Unified function to handle different file types\n",
    "def extract_text_from_file(file_path):\n",
    "    extension = os.path.splitext(file_path)[1].lower()\n",
    "    \n",
    "    if extension == '.pdf':\n",
    "        return extract_text_from_pdf(file_path)\n",
    "    elif extension == '.docx':\n",
    "        return extract_text_from_word(file_path)\n",
    "    elif extension == '.xlsx':\n",
    "        return extract_text_from_excel(file_path)\n",
    "    elif extension == '.pptx':\n",
    "        return extract_text_from_ppt(file_path)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file format: {extension}\")\n",
    "\n",
    "# Clean and preprocess the extracted text\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'<.*?>', '', text)  # Remove HTML tags\n",
    "    text = re.sub(r'\\[.*?\\]', '', text)  # Remove square brackets content\n",
    "    text = re.sub(r'[^a-zA-Z0-9.,!?\\'\\s]', '', text)  # Remove special characters\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces with single space\n",
    "    return text.strip()\n",
    "\n",
    "# Example usage: Handling multiple files\n",
    "file_paths = [\n",
    "    \"C:/Users/grn61/OneDrive/Desktop/Applied Spectroscopy/Donald L. Pavia, Gary M. Lampman, George S. Kriz, James R. Vyvyan - Introduction to Spectroscopy-Cengage Learning (c2015).pdf\", \n",
    "    ]\n",
    "\n",
    "corpus = ''\n",
    "for file_path in file_paths:\n",
    "    corpus += extract_text_from_file(file_path)\n",
    "\n",
    "cleaned_corpus = clean_text(corpus)\n",
    "\n",
    "print(\"Extracted and Cleaned Text:\", cleaned_corpus[:1000])  # Print the first 500 characters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "54ab95dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Tokenization and transformer based language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17b59ba6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\grn61\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "C:\\Users\\grn61\\anaconda3\\Lib\\site-packages\\torch\\nn\\functional.py:5560: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 9.5966\n",
      "Epoch [2/10], Loss: 8.3256\n",
      "Epoch [3/10], Loss: 7.9090\n",
      "Epoch [4/10], Loss: 7.7104\n",
      "Epoch [5/10], Loss: 7.4505\n",
      "Epoch [6/10], Loss: 7.2340\n",
      "Epoch [7/10], Loss: 7.0184\n",
      "Epoch [8/10], Loss: 6.7732\n",
      "Epoch [9/10], Loss: 6.5030\n",
      "Epoch [10/10], Loss: 6.2421\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tempfile\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import math\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tokenizers import Tokenizer, models, trainers, pre_tokenizers, decoders\n",
    "from tokenizers.normalizers import BertNormalizer\n",
    "\n",
    "# Function to train and load a WordPiece tokenizer\n",
    "def train_tokenizer_from_corpus(cleaned_corpus, save_path=\"./tokenizer\"):\n",
    "    \"\"\"Train a WordPiece tokenizer on the cleaned corpus and save the model.\"\"\"\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "    # Save the cleaned corpus to a temporary file\n",
    "    temp_file_path = os.path.join(save_path, \"temp.txt\")\n",
    "    with open(temp_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(cleaned_corpus)\n",
    "\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = Tokenizer(models.WordPiece(unk_token=\"[UNK]\"))\n",
    "    tokenizer.normalizer = BertNormalizer()\n",
    "    tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "\n",
    "    # Create a trainer\n",
    "    trainer = trainers.WordPieceTrainer(vocab_size=30000, min_frequency=2, special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
    "    \n",
    "    # Train the tokenizer\n",
    "    tokenizer.train([temp_file_path], trainer)\n",
    "\n",
    "    # Save the tokenizer\n",
    "    tokenizer.save(os.path.join(save_path, \"tokenizer.json\"))\n",
    "\n",
    "    return tokenizer\n",
    "\n",
    "# Function to encode sentences using the trained tokenizer\n",
    "def encode_sentences(tokenizer, sentences):\n",
    "    return [tokenizer.encode(sentence).ids for sentence in sentences]\n",
    "\n",
    "# Dataset class for tokenized sequences with efficient padding\n",
    "class TokenizedDataset(Dataset):\n",
    "    def __init__(self, tokenized_sequences, max_seq_length):\n",
    "        self.sequences = tokenized_sequences\n",
    "        self.max_seq_length = max_seq_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.sequences[idx]\n",
    "        seq = seq[:self.max_seq_length] + [0] * (self.max_seq_length - len(seq))  # Padding\n",
    "        return torch.tensor(seq)\n",
    "\n",
    "# Positional encoding for the transformer model\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:x.size(0), :]\n",
    "\n",
    "# Transformer Model definition\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, max_seq_length):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, max_seq_length)\n",
    "        self.transformer = nn.Transformer(d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward)\n",
    "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src = self.embedding(src) * math.sqrt(self.embedding.weight.size(1))\n",
    "        tgt = self.embedding(tgt) * math.sqrt(self.embedding.weight.size(1))\n",
    "        src = self.pos_encoder(src)\n",
    "        tgt = self.pos_encoder(tgt)\n",
    "\n",
    "        output = self.transformer(src, tgt)\n",
    "        return self.fc_out(output)\n",
    "\n",
    "# Training function\n",
    "def train_model(model, tokenizer, cleaned_corpus, batch_size=32, epochs=10, max_seq_length=512, device='cuda'):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "    # Tokenizing the cleaned corpus\n",
    "    tokenized_sequences = [tokenizer.encode(sentence).ids for sentence in cleaned_corpus.split('\\n') if sentence]\n",
    "    \n",
    "    if not tokenized_sequences:\n",
    "        print(\"No sentences to train on. Please check the cleaned corpus.\")\n",
    "        return\n",
    "\n",
    "    dataset = TokenizedDataset(tokenized_sequences, max_seq_length)\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch in data_loader:\n",
    "            batch = batch.to(device)\n",
    "            src = batch[:, :-1]\n",
    "            tgt = batch[:, 1:]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(src, tgt)\n",
    "\n",
    "            loss = criterion(output.view(-1, output.size(-1)), tgt.view(-1))\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {total_loss / len(data_loader):.4f}\")\n",
    "\n",
    "# Save the trained model\n",
    "def save_model(model, filepath):\n",
    "    torch.save(model.state_dict(), filepath)\n",
    "\n",
    "# Load the trained model\n",
    "def load_model(filepath, vocab_size, d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, max_seq_length):\n",
    "    model = TransformerModel(vocab_size, d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, max_seq_length)\n",
    "    model.load_state_dict(torch.load(filepath))\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    return model\n",
    "\n",
    "# Example cleaned corpus string\n",
    "#cleaned_corpus = \"\"\"INTRODUCTION TO SPECTROSCOPY Donald L. Pavia Gary M. Lampman George S. Kriz James R. Vyvyan Department of Chemistry Western Washington University Bellingham, Washington FIFTH EDITION Australia Brazil Mexico Singapore United Kingdom United States Copyright 2013 Cengage Learning. All Rights Reserved. May not be copied, scanned, or duplicated, in whole or in part.\"\"\"\n",
    "\n",
    "# Train the tokenizer\n",
    "tokenizer = train_tokenizer_from_corpus(cleaned_corpus)  \n",
    "if tokenizer is None:\n",
    "    print(\"Tokenizer training failed.\")\n",
    "else:\n",
    "    # Set model parameters\n",
    "    vocab_size = tokenizer.get_vocab_size()\n",
    "    d_model = 1024\n",
    "    nhead = 16\n",
    "    num_encoder_layers = 12\n",
    "    num_decoder_layers = 12\n",
    "    dim_feedforward = 4096\n",
    "    max_seq_length = 512\n",
    "\n",
    "    # Create and train the model\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model = TransformerModel(vocab_size, d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, max_seq_length)\n",
    "    train_model(model, tokenizer, cleaned_corpus, batch_size=32, epochs=10, max_seq_length=max_seq_length, device=device)\n",
    "\n",
    "    # Save the trained model\n",
    "    save_model(model, \"transformer_model.pth\")\n",
    "\n",
    "    # Load the model later if needed\n",
    "    # model = load_model(\"transformer_model.pth\", vocab_size, d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, max_seq_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b1b5d0eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Locations and Frequencies:\n",
      "print: Frequency = 33, Locations = [661, 64406, 201600, 202178, 239572, 253029, 253691, 261473, 265511, 275482, 276724, 457338, 489791, 630878, 724966, 725096, 725176, 740082, 742239, 873447, 873855, 883791, 888419, 888898, 1033381, 1035243, 1055775, 1063158, 1072805, 1073875, 1074257, 1142996, 1157340]\n",
      "title: Frequency = 8, Locations = [937, 1173, 8439, 8471, 8745, 10963, 10995, 1020518]\n",
      "may: Frequency = 258, Locations = [741, 1452, 6143, 6540, 25964, 27300, 29582, 30179, 32057, 32079, 36625, 37668, 38008, 39691, 41091, 41141, 44153, 48846, 55903, 60117, 62019, 69237, 70684, 74332, 77883, 78238, 87966, 90552, 94095, 94569, 100953, 101230, 110282, 111337, 111603, 112282, 113099, 113150, 114556, 122594, 126757, 129699, 134997, 135720, 143237, 143486, 143859, 153530, 158433, 159942, 160098, 160378, 170413, 175753, 179212, 182327, 182894, 183312, 187574, 194085, 203788, 206610, 212990, 215016, 217772, 219494, 219664, 226641, 240018, 242451, 245785, 247329, 255426, 258352, 262657, 263526, 270852, 277915, 281656, 284609, 292450, 293431, 298847, 299861, 300454, 303418, 305622, 306079, 309971, 311190, 311605, 312010, 319070, 323763, 323812, 323819, 337066, 340548, 350236, 350988, 351578, 354326, 358313, 358786, 358990, 361117, 364045, 366194, 367703, 370119, 372037, 373338, 376245, 377077, 378863, 381816, 383335, 383871, 386234, 387560, 389654, 391221, 396294, 396356, 401492, 417866, 419195, 420756, 453322, 455931, 491518, 494669, 495532, 497089, 498027, 500296, 500349, 500535, 509320, 511549, 511590, 512368, 524402, 525757, 525967, 528024, 535877, 536759, 557323, 582984, 587388, 590227, 606708, 610933, 615445, 632591, 632752, 659344, 671018, 671362, 693413, 708391, 709033, 709278, 718244, 719834, 726702, 733288, 734844, 736111, 738408, 739438, 744332, 757098, 757191, 763822, 787857, 791976, 792014, 792190, 793338, 806012, 820704, 826717, 866942, 868217, 868977, 870555, 878354, 885520, 899051, 905538, 910682, 931606, 931631, 931680, 932352, 932534, 945239, 959858, 959932, 960012, 960892, 961430, 965985, 966355, 971213, 971282, 971767, 972410, 976707, 981052, 990809, 992108, 995941, 999975, 1010437, 1021896, 1023691, 1024868, 1026813, 1028093, 1028581, 1029289, 1034873, 1035907, 1039271, 1040292, 1040595, 1040956, 1052550, 1062937, 1066171, 1066974, 1073509, 1075929, 1079121, 1081979, 1082275, 1082816, 1083321, 1083872, 1083984, 1084257, 1084543, 1084570, 1089629, 1089748, 1090449, 1090753, 1091530, 1091829, 1094380, 1094766, 1095581, 1096151, 1096340, 1162346]\n",
      "Predicted next word: or\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Function to find frequency and locations of user-defined words/phrases\n",
    "def find_word_locations(cleaned_corpus, user_words):\n",
    "    word_locations = {}\n",
    "    for word in user_words:\n",
    "        locations = []\n",
    "        start = 0\n",
    "        while True:\n",
    "            start = cleaned_corpus.find(word, start)\n",
    "            if start == -1:\n",
    "                break\n",
    "            locations.append(start)\n",
    "            start += len(word)  # Move past the last found word\n",
    "        word_locations[word] = {\n",
    "            'frequency': len(locations),\n",
    "            'locations': locations\n",
    "        }\n",
    "    return word_locations\n",
    "\n",
    "# Function to predict the next word\n",
    "def predict_next_word(model, tokenizer, input_sequence, device='cuda'):\n",
    "    model.eval()\n",
    "    # Tokenize the input sequence\n",
    "    tokenized_input = tokenizer.encode(input_sequence).ids\n",
    "    input_tensor = torch.tensor(tokenized_input).unsqueeze(0).to(device)\n",
    "\n",
    "    # Prepare source and target for prediction\n",
    "    src = input_tensor\n",
    "    tgt = input_tensor  # Here, you can use the same tensor for simplicity\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(src, tgt)\n",
    "    \n",
    "    # Get the last predicted word\n",
    "    last_word_logits = output[-1, -1, :]  # Last time step, last token\n",
    "    predicted_word_index = last_word_logits.argmax().item()\n",
    "    predicted_word = tokenizer.decode([predicted_word_index])\n",
    "\n",
    "    return predicted_word\n",
    "\n",
    "# Example cleaned corpus string (make sure it is set correctly)\n",
    "# cleaned_corpus = \"...\" \n",
    "\n",
    "# Define your words/phrases to search\n",
    "user_words = [\"print\", \"title\", \"may\"]\n",
    "\n",
    "# Find frequency and locations\n",
    "word_locations = find_word_locations(cleaned_corpus, user_words)\n",
    "print(\"Word Locations and Frequencies:\")\n",
    "for word, info in word_locations.items():\n",
    "    print(f\"{word}: Frequency = {info['frequency']}, Locations = {info['locations']}\")\n",
    "\n",
    "# Example of predicting the next word\n",
    "input_sequence = \"mass spectroscopy\"  # Define the sequence\n",
    "predicted_word = predict_next_word(model, tokenizer, input_sequence, device)\n",
    "print(f\"Predicted next word: {predicted_word}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c49230f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
